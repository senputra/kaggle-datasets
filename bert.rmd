```{r}

library(dplyr)
library(tidyverse)
library(zeallot)
library(reticulate)
library(keras)
library(tensorflow)

Sys.setenv(TF_KERAS=1)
# Sys.setenv(KERAS_BACKEND="plaidml.keras.backend")
use_python('/Users/dodysenputra/opt/anaconda3/envs/python/bin/python', required=T)
reticulate::py_config() # check if python is version >3

```



```{r}
py_module_available('keras_bert')
# tensorflow::install_tensorflow(version = "1.15")
```

```{r}
pretrained_path = '/Users/dodysenputra/Projects/kaggle/test'
config_path = file.path(pretrained_path, 'bert_config.json')
checkpoint_path = file.path(pretrained_path, 'bert_model.ckpt')
vocab_path = file.path(pretrained_path, 'vocab.txt')

k_bert = import('keras_bert')
token_dict = k_bert$load_vocabulary(vocab_path)
tokenizer = k_bert$Tokenizer(token_dict)
```

Prepare functions
```{r}
# tokenize text
tokenize_fun = function(dataset) {
  # print(dataset)
  c(indices, target, segments) %<-% list(list(), list(), list())
  for (i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i],
                                                       max_len = seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    target = target %>% append(dataset[[LABEL_COLUMN]][i])
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices, segments, target))
}
# read data
dt_data = function(dir, rows_to_read) {
  data = data.table::fread(dir, nrows = rows_to_read)
  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)
  return(list(x_train, x_segment, y_train))
}



# tokenize text
tokenize_fun_test = function(dataset) {
  # print(dataset)
  c(indices, segments) %<-% list(list(), list())
  for (i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i],
                                                       max_len = seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices, segments))
}
dt_data_test = function(dir, rows_to_read) {
  data = data.table::fread(dir, nrows = rows_to_read)
  c(x_train, x_segment) %<-% tokenize_fun_test(data)
  return(list(x_train, x_segment))
}
```
some constants to load models
```{r}
seq_length = 50L
bch_size = 2^5 #128
epochs = 1
learning_rate = 1e-4

DATA_COLUMN = 'tweet'
LABEL_COLUMN = 'sentiment'
```
Initiate model
```{r}
model = k_bert$load_trained_model_from_checkpoint(
  config_path,
  checkpoint_path,
  training=T,
  trainable=T,
  seq_len=seq_length)
```

```{r}
# 22500
c(x_train, x_segment, y_train) %<-%
  dt_data(file.path(pretrained_path, 'train.csv'), 22500)
```

matrix format for keras-bert
```{r}
factorise <- function(data, ncol) {
  parsed = c()
  for (i in 1:length(data)) {
    x = data[i]
    parsed = c(parsed, c(
      if (x == 1) 1 else 0, 
      if (x == 2) 1 else 0, 
      if (x == 3) 1 else 0)
      )
  }
  return(matrix(parsed, ncol=ncol,byrow=TRUE))
}

train = do.call(cbind,x_train) %>% t()
segments = do.call(cbind,x_segment) %>% t()
targets = do.call(cbind,y_train) %>% t() %>% factorise(3)
concat = c(list(train),list(segments))

```

```{r}
c(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(
  targets %>% length(),
  batch_size=bch_size,
  epochs=epochs
)
```


Determine inputs and outputs, then concatenate them
```{r}

input_1 = get_layer(model,name = 'Input-Token')$input
input_2 = get_layer(model,name = 'Input-Segment')$input
inputs = list(input_1,input_2)

dense = get_layer(model,name = 'NSP-Dense')$output

outputs = dense %>% layer_dense(
  units = 5L,
  activation = 'sigmoid',
  kernel_initializer = initializer_truncated_normal(stddev = 0.02),
  name = 'output'
) %>% layer_dense(
  units = 3L,
  activation = 'sigmoid',
  kernel_initializer = initializer_truncated_normal(stddev = 0.02),
  name = 'output-2'
)

model = keras_model(inputs = inputs,outputs = outputs)
```

```{r}
outputResult <- function(fileName, epochsCount) {
  test_data_count = 7500
  c(x_test, x_segment) %<-%
    dt_data_test(file.path(pretrained_path, 'test.csv'), test_data_count)
  test = do.call(cbind, x_test) %>% t()
  segments = do.call(cbind, x_segment) %>% t()
  concat = c(list(test), list(segments))
  
  prediction <- model %>% predict(concat)
  output <- prediction %>% apply(FUN = which.max, MARGIN = 1)
  output <- data.frame(id = 1:test_data_count, sentiment = output)
  write.csv(output, paste0(fileName,'_epochs ', epochsCount, '_full data.csv'),row.names = FALSE)
}
```



fit()

```{r}
model %>% compile(
  k_bert$AdamWarmup(decay_steps=decay_steps, 
                    warmup_steps=warmup_steps, lr=learning_rate),
  loss = 'binary_crossentropy',
  metrics = 'accuracy'
)
for (i in 1:1) {
 history <- model %>% fit(
    concat,
    targets,
    epochs = epochs,
    batch_size = bch_size,
    validation_split = 0.2
  )
  save_model_hdf5(model, paste0('model_epochs ', i,'.h5'))
  outputResult('selfRun_5 dec', i)
}


```


```{r}
# load Keras and predict
loaded_model = keras_model(inputs = inputs,outputs = outputs)
loaded_model %>% load_model_weights_hdf5('model_epochs 5.h5')

c(x_test, x_segment_test) %<-%
  dt_data_test(file.path(pretrained_path, 'test.csv'), 5)
test = do.call(cbind, x_test) %>% t()
segments_test = do.call(cbind, x_segment_test) %>% t()
concat = c(list(test), list(segments_test))

loaded_model %>% predict(concat)
```






```{r}

output1 <- read.csv(paste0('selfRun_5 dec_epochs ', 10,'_full data.csv'))
output2 <- read.csv(paste0('selfRun_5 dec_epochs ', 9,'_full data.csv'))
table(output1 == output2)

```